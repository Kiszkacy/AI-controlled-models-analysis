environment:
  observation_space_size: 5 # Dimensionality of the observation space, number of features in each state observation.
  observation_space_low: -100 # Lower bound for each dimension in the observation space.
  observation_space_high: 100 # Upper bound for each dimension in the observation space.
  action_space_range: 2  # Number of possible actions the agent can take in the environment
  action_space_low: -1 # Lower bound for each dimension in the action space.
  action_space_high: 1 # Upper bound for each dimension in the action space.
  number_of_agents: 25 # Number af agents in environment starting
training:
  number_of_workers: 5  # Number of parallel processes to use for training.
  number_of_env_per_worker: 1  # Number of training environments simulated by each worker process.
  training_iterations: 2000  # Total number of training iterations (over a) to perform.
  training_batch_size: 100 # 2048 # Total amount of environment steps taken during each training iteration
  training_checkpoint_frequency: 10 # Number of training steps how often Tuner is saved
  use_gpu: False # Defines if gpu should be used
  algorithm: "PPO" # Algorithm used for training, works with PPO, DQN, SAC
  is_resume: False  # Defines if it's a resume of training or not
  base_storage_dir: "C:\\Users\\sokol\\PycharmProjects\\AI-controlled-models-analysis\\core\\src\\model" # Dir where the models are saved
  save_dir: "test_3"  # Dir in the base_storage dir where the current algorithm will be saved/be restored from
  max_checkpoints: 5  # Num of checkpoints saved
  restore_iteration:  # If we want to restore model from "special" iteration we need to provide it here (needs to be in the saved iterations!),
  # by default empty/None restarts training from the latest checkpoint
